#!/bin/sh
#
# ScaleIO FlexVolume driver
#
# Do not modify thist script, configuration settings can be found at
# /opt/emc/scaleio/flexvolume/cfg/config

# stderr
err() {
	debug $*
	shift
	printf "%b" "$*" 1>&2
}

# stdout
log() {
	debug $*
	shift
	printf "%b" "$*" >&1
}

# log to file (if debug)
debug() {
	if [ "$SCALEIO_DEBUG" = "1" ]; then
		logtofile $*
	fi
}

# log to file
logtofile() {
	function=$1
	shift
	printf "%s pid=%s func=%s msg=\"%s\"\n" "$(date +"%Y-%m-%d %H:%M:%S")" "$$" "${function}" "$*" >> ${LOGFILE}
}

success() {
	log $1 '{"status": "Success"}'
	exit 0
}

usage() {
	err ${FUNCNAME} "error: invalid usage\n"
	err ${FUNCNAME} "\t$0 init\n"
	err ${FUNCNAME} "\t$0 attach <json params> <nodename>\n"
	err ${FUNCNAME} "\t$0 detach <mount device> <nodename>\n"
	err ${FUNCNAME} "\t$0 waitforattach <mount device> <json params>\n"
	err ${FUNCNAME} "\t$0 mountdevice <mount dir> <mount device> <json params>\n"
	err ${FUNCNAME} "\t$0 unmountdevice <mount dir>\n"
	err ${FUNCNAME} "\t$0 isattached <json params> <nodename>\n"

	exit 1
}

ismounted() {
	local mntpath=$1
	local mount=$(findmnt -n "${mntpath}" 2>/dev/null | cut -d' ' -f1)
	if [ "${mount}" = "${mntpath}" ]; then echo 1; else echo 0; fi
}

volidfromjson() {
	local json_params=$1
	local volumeid=$(echo "${json_params}" | grep -Po '"volumeID":".*"?[^\\]"' | cut -d: -f 2 | cut -d\" -f 2)
	if [ -z "${volumeid}" ] || [ "${volumeid}" = null ]; then
		err ${FUNCNAME} "{\"status\": \"Failure\", \"message\": \"Unable to extract volumeID\"}"
		exit 1
	fi
	echo ${volumeid}
}

getvolumename() {
	local json_params=$1
	local volumeid=$(echo "${json_params}" | grep -Po '"volumeID":".*"?[^\\]"' | cut -d: -f 2 | cut -d\" -f 2)
	if [ -z "${volumeid}" ] || [ "${volumeid}" = null ]; then
		err ${FUNCNAME} "{\"status\": \"Failure\", \"message\": \"Unable to extract volumeID\"}"
		exit 1
	fi
	log ${FUNCNAME} "{\"status\": \"Success\", \"volumeName\": \"${volumeid}\"}"
	exit 0
}

attach() {
	local json_params=$1
	local node_name=$2
	local volumeid=$(volidfromjson ${json_params})

	if [ -z "${volumeid}" ]; then
		err ${FUNCNAME} '{"status": "Failure", "message": "Unable to extract volumeID"}'
		exit 1
	fi

	local volume_attached=$(${CLI_CMD} ${CONFIG_OPTION}=${KUBECONFIG} --token=${TOKEN} get pv ${volumeid} --template='{{.metadata.labels.attached}}')
	local volume_attached_node=$(${CLI_CMD} ${CONFIG_OPTION}=${KUBECONFIG} --token=${TOKEN} get pv ${volumeid} --template='{{.metadata.labels.attachedNode}}')
	debug ${FUNCNAME} "attached:" "${volume_attached}"
	debug ${FUNCNAME} "attachedNode:" "${volume_attached_node}"

	if [ -z "${volume_attached}" ]; then
		volume_attached="false"
	fi
	if [ "${volume_attached}" = "true" ]; then
		if [ "${volume_attached_node}" = "${node_name}" ]; then
			log ${FUNCNAME} "{\"status\": \"Success\", \"device\":\"${volumeid}\"}"
			exit 0
		fi
		err ${FUNCNAME} "{\"status\": \"Failure\", \"message\": \"Volume is in use by another node\"}"
		exit 1
	fi

	local set_label_attached=$(${CLI_CMD} ${CONFIG_OPTION}=${KUBECONFIG} --token=${TOKEN} label pv ${volumeid} --overwrite attached=true)
	local set_local_attached_node=$(${CLI_CMD} ${CONFIG_OPTION}=${KUBECONFIG} --token=${TOKEN} label pv ${volumeid} --overwrite attachedNode=${node_name})
	debug ${FUNCNAME} "set_label_attached:" "${set_label_attached}"
	debug ${FUNCNAME} "set_label_attachedNode:" "${set_label_attached_node}"

	log ${FUNCNAME} "{\"status\": \"Success\", \"device\":\"${volumeid}\"}"
	exit 0
}

detach() {
	local volumeid=$1

	local set_label_attached=$(${CLI_CMD} ${CONFIG_OPTION}=${KUBECONFIG} --token=${TOKEN} label pv ${volumeid} --overwrite attached=false)
	local set_label_attached_node=$(${CLI_CMD} ${CONFIG_OPTION}=${KUBECONFIG} --token=${TOKEN} label pv ${volumeid} --overwrite attachedNode-)
	debug ${FUNCNAME} "set_label_attached:" "${set_label_attached}"
	debug ${FUNCNAME} "set_label_attachedNode:" "${set_label_attached_node}"

	success ${FUNCNAME}
}

isvolumeinuse() {
	local volumeid=$1
	local running_pods=$(${CLI_CMD} ${CONFIG_OPTION}=${KUBECONFIG} --token=${TOKEN} get pods | grep Running | awk '{print $1}')
	debug ${FUNCNAME} "running pods:" "${running_pods}"
	local i
	for i in ${running_pods}
	do
		local flex_volume_ids=$(${CLI_CMD} ${CONFIG_OPTION}=${KUBECONFIG} --token=${TOKEN} get pod ${i} --template='{{range .spec.volumes}}{{.flexVolume.options.volumeID}} {{end}}' | sed -e 's/<no value>//g')
		debug ${FUNCNAME} "FlexVolumes attached to pod ${i}:" "${flex_volume_ids}"
		local j
		for j in ${flex_volume_ids}
		do
			if [ "${j}" = "${volumeid}" ]; then
				debug ${FUNCNAME} "Volume ${volumeid} is in use by another pod ${i}"
				echo 1
				return
			fi
		done

		local pvcs=$(${CLI_CMD} ${CONFIG_OPTION}=${KUBECONFIG} --token=${TOKEN} get pod ${i} --template='{{range .spec.volumes}}{{.persistentVolumeClaim.claimName}} {{end}}' | sed -e 's/<no value>//g')
		debug ${FUNCNAME} "PVCs attached to pod ${i}:" "${pvcs}"
		local k
		for k in ${pvcs}
		do
			if [ "${k}" != "<no value>" ]; then
				local pvc_vol_names=$(${CLI_CMD} ${CONFIG_OPTION}=${KUBECONFIG} --token=${TOKEN} get pvc ${k} --template='{{.spec.volumeName}}')
				local m
				for m in ${pvc_vol_names}
				do
					if [ "${m}" = "${volumeid}" ]; then
						debug ${FUNCNAME} "Volume ${volumeid} claimed by PVC ${k} is in use by another pod ${i}"
						echo 1
						return
					fi
				done
			fi
		done
	done

	echo 0
	return
}

mountdevice() {
	local mntpath=$1
	local dev=$2
	local json_params=$3
	local fstype=$(echo "${json_params}" | grep -Po '"kubernetes.io/fsType":".*"?[^\\]"' | cut -d: -f 2 | cut -d\" -f 2)

	if [ ! -b "${dev}" ]; then
		err ${FUNCNAME} "{\"status\": \"Failure\", \"message\": \"${dev} does not exist\"}"
		exit 1
	fi

	if [ "$(ismounted ${mntpath})" = "1" ] ; then
		success ${FUNCNAME}
	fi

	local volfstype=$(blkid -o udev "${dev}" 2>/dev/null | grep "ID_FS_TYPE"| cut -d"=" -f2)
	if [ "${volfstype}" = "" ]; then
		local cmd="mkfs -t ${fstype}"
		if [ "${fstype}" = "ext4" ]; then
			cmd="${cmd} -F"
		elif [ "$FSTYPE" = "xfs" ]; then
			cmd="${cmd} -f"
		fi
		if ! ${cmd} "${dev}" > /dev/null 2>&1; then
			err ${FUNCNAME} "{\"status\": \"Failure\", \"message\": \"Failed to create fs ${fstype} on device ${dev}\"}"
			exit 1
		fi
	fi

	mkdir -p "${mntpath}" > /dev/null 2>&1
	local selinux_status=$(sestatus |grep 'SELinux status' | grep disabled)
	local selinux_enabled=$(echo $?)
	if ! mount --make-shared "${dev}" "${mntpath}" > /dev/null 2>&1; then
		debug ${FUNCNAME} "Failed to mount device ${dev} at ${mntpath}"
		err ${FUNCNAME} "{\"status\": \"Failure\", \"message\": \"Failed to mount device ${dev} at ${mntpath}\"}"
		exit 1
	fi
	if [ "${selinux_enabled}" = "1" ]; then
		if ! chcon -t svirt_sandbox_file_t "${mntpath}" > /dev/null 2>&1; then
			debug ${FUNCNAME} "Failed to modify SELINUX properties for ${mntpath}"
			err ${FUNCNAME} "{\"status\": \"Failure\", \"message\": \"Failed to modify SELINUX properties for ${mntpath}\"}"
			exit 1
		fi
	fi
	success ${FUNCNAME}
}

unmountdevice() {
	local mntpath=$1
	if [ "$(ismounted ${mntpath})" = "0" ] ; then success ${FUNCNAME}; fi
	if ! umount "${mntpath}" > /dev/null 2>&1; then
		err ${FUNCNAME} "{ \"status\": \"Failed\", \"message\": \"Failed to unmount volume at ${mntpath}\"}"
		exit 1
	fi
	sleep 2
	success ${FUNCNAME}
}

isattached() {
	local json_params=$1
	local node_name=$2
	local volumeid=$(volidfromjson ${json_params})

	local volume_attached=$(${CLI_CMD} ${CONFIG_OPTION}=${KUBECONFIG} --token=${TOKEN} get pv ${volumeid} --template='{{.metadata.labels.attached}}')
	local volume_attached_node=$(${CLI_CMD} ${CONFIG_OPTION}=${KUBECONFIG} --token=${TOKEN} get pv ${volumeid} --template='{{.metadata.labels.attachedNode}}')
	debug ${FUNCNAME} "attached:" "${volume_attached}"
	debug ${FUNCNAME} "attachedNode:" "${volume_attached_node}"

	if [ "${volume_attached}" = "true" ]; then
		if [ "${volume_attached_node}" = "${node_name}" ]; then
			log ${FUNCNAME} "{\"status\": \"Success\", \"attached\":true}"
			exit 0
		fi
	fi
	log ${FUNCNAME} "{\"status\": \"Success\", \"attached\":false}"
	exit 0
}

waitforattach() {
	local expected_dev=$1
	local json_params=$2
	local volumeid=$(volidfromjson ${json_params})

	local dev="/dev/disk/by-id/$(ls -1 /dev/disk/by-id | grep "\-${volumeid}$")"

	if [ -z "${dev}" ]; then
		debug ${FUNCNAME} "${volumeid}" "not found or mapped"
		err ${FUNCNAME} "{\"status\": \"Failure\", \"message\": \"Volume not attached\"}"
		exit 1
	fi

	if [ "${ENABLE_ONE_POD_RESTRICTION}" = "1" ]; then
		local vol_attached_to_another_pod=$(isvolumeinuse ${volumeid})
		debug ${FUNCNAME} "vol_attached_to_another_pod:" "${vol_attached_to_another_pod}"
		if [ "${vol_attached_to_another_pod}" = "1" ]; then
			err ${FUNCNAME} "{\"status\": \"Failure\", \"message\": \"Volume ${volumeid} is in use by another pod\"}"
			exit 1
		fi
	fi

	log ${FUNCNAME} "{\"status\": \"Success\", \"device\":\"${dev}\"}"
	exit 0
}

# MAIN

CONFIGFILE="/opt/emc/scaleio/flexvolume/cfg/config"
if [ -f "${CONFIGFILE}" ]; then
	# source the config file
	source "${CONFIGFILE}"
fi

# in case some values were not specific in the config file, set their values
SCALEIO_DEBUG="${SCALEIO_DEBUG:-1}"
KUBECONFIG="${KUBECONFIG:-/root/.kube/config}"
TOKEN_SCRIPT="${TOKEN_SCRIPT:-/opt/emc/scaleio/flexvolume/bin/get-token.sh}"
LOGFILE="${LOGFILE:-/var/log/scaleio-flexvol.log}"
ENABLE_ONE_POD_RESTRICTION="${ENABLE_ONE_POD_RESTRICTION:-0}"

op=$1

if [ "$op" = "init" ]; then
	log "main" "{\"status\":\"Success\",\"capabilities\":{\"attach\":true}}"
	exit 0
fi
if [ "$#" -lt "2" ]; then usage; fi

shift

debug "main" "$op" "$*"

TOKEN="$(${TOKEN_SCRIPT})"

WHICH_OC=$(which oc > /dev/null 2>&1)
WHICH_OC_RETURN=$(echo $?)
if [ "${WHICH_OC_RETURN}" = "0" ]; then
	CLI_CMD="oc"
	CONFIG_OPTION="--config"
else
	CLI_CMD="kubectl"
	CONFIG_OPTION="--kubeconfig"
fi

case "$op" in
	attach)
		attach "$@"
		;;
	detach)
		detach "$@"
		;;
	waitforattach)
		waitforattach "$@"
		;;
	mountdevice)
		mountdevice "$@"
		;;
	unmountdevice)
		unmountdevice "$@"
		;;
	isattached)
		isattached "$@"
		;;
	getvolumename)
		getvolumename "$@"
		;;
	*)
		err "main" "{\"status\": \"Not supported\"}"
		exit 1
esac

exit 1
